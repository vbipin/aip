{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mdp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vbipin/aip/blob/master/mdp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr2dA9HmkePh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we plan to implement some of the algorithms related to MDPs and RL\n",
        "#MDP study\n",
        "#%matplotlib inline\n",
        "#import matplotlib\n",
        "#import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#I am trying to avoid the numpy dependencies\n",
        "\n",
        "import random\n",
        "#\n",
        "#We plan to implement the gridworld class \n",
        "#\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NU8O0Do5k-8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Let us have a gridworld\n",
        "#ref: Chapter 17, Artificial Intelligence a Modern Approach\n",
        "#ref: CS188 https://inst.eecs.berkeley.edu/~cs188/fa19/\n",
        "#ref: https://inst.eecs.berkeley.edu/~cs188/fa19/assets/slides/lec8.pdf\n",
        "#ref: https://courses.cs.washington.edu/courses/cse473/13au/slides/17-mdp-rl.pdf\n",
        "\n",
        "#This class will create a 2D grid of row x colums \n",
        "#Some of the cells can be disabled by putting it into walls\n",
        "#cells are addressed just like 2d arrays (r,c)\n",
        "#There are possibly many terminal states\n",
        "#terminal states have only one action available: Exit \n",
        "#Transistion is as per the book 80% action and 20%sideways ( a variable noise is used to control this distribution)\n",
        "#There is a special end state, (-1,-1), from which NO action is available. This state is used as a final state.\n",
        "\n",
        "#Actions #just some alias\n",
        "Up    = 0\n",
        "Down  = 1\n",
        "Right = 2\n",
        "Left  = 3\n",
        "Exit  = 4\n",
        "\n",
        "class GridWorld :\n",
        "    #Default is as given in the AIMA book\n",
        "    def __init__(self, \n",
        "                 rows    =3, \n",
        "                 columns =4, \n",
        "                 walls   =[(1,1)], terminals= {(0,3):+1.0, (1,3):-1.0}, \n",
        "                 gamma   =1.0, \n",
        "                 living_reward=0,\n",
        "                 noise   =0.2\n",
        "                ) :\n",
        "        \"\"\"We dont expect these parameters to change during the agent run\"\"\"\n",
        "        self.rows      = rows\n",
        "        self.columns   = columns\n",
        "        self.N         = rows * columns #total cells\n",
        "        self.walls     = walls\n",
        "        self.terminals = terminals #dictionary of terminal celss and their rewards.\n",
        "        self.gamma     = gamma\n",
        "        self.living_reward = living_reward\n",
        "        self.all_actions   = [ Up, Down, Right, Left, Exit ]\n",
        "        self.end_state     = (-1, -1) #a dummy state to reach after taking Exit\n",
        "        self.all_states    = [(r,c) for r in range(rows) for c in range(columns) if (r,c) not in walls ] + [self.end_state]\n",
        "        self.noise         = noise\n",
        "        \n",
        "        \n",
        "        #transitions from each state and the probabilities\n",
        "        self.noise                = noise\n",
        "        self.action_transitions   = { \n",
        "            Up:   ([Up,    Left, Right], [1-noise, noise/2, noise/2 ]),\n",
        "            Down: ([Down,  Left, Right], [1-noise, noise/2, noise/2 ]),\n",
        "            Left: ([Left,  Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
        "            Right:([Right, Up,   Down ], [1-noise, noise/2, noise/2 ]),\n",
        "            Exit :([Exit], [1.0])\n",
        "        }\n",
        "    \n",
        "    def actions(self, state) :\n",
        "        \"\"\"returns all valid actions from the current state\"\"\"\n",
        "        if state in self.terminals :\n",
        "            return [Exit]\n",
        "        if state == self.end_state :\n",
        "            return [] #No action available.\n",
        "        return [ Up, Down, Right, Left ]\n",
        "    \n",
        "    def reward(self, state, action, next_state=None) :\n",
        "        \"\"\"reward is the instantaneous reward. It is usually R(s,a,s')\"\"\"\n",
        "        #In grid world the reward depends only on state.\n",
        "        if state in self.terminals :\n",
        "            return self.terminals[state] #dict has the terminal values +1 or -1\n",
        "        if state == self.end_state :\n",
        "            return 0.0\n",
        "        return self.living_reward        #usually a small -ve value\n",
        "    \n",
        "    def transitions(self, state, action) :\n",
        "        \"\"\"returna list of tuple(nextstate, action, probability)\"\"\"\n",
        "        actual_actions, probs = self.action_transitions[action]\n",
        "        return [ self._next_cell(state, a) for a in actual_actions ], actual_actions, probs\n",
        "    \n",
        "    def move(self, state, action) :\n",
        "        \"\"\"Take the action and return the tuple(new_state, reward, is_terminal)\"\"\"                          \n",
        "        assert action in self.actions(state) #just a check if this is a valid action at this time or not\n",
        "        \n",
        "        cells, actions, p = self.transitions(state, action)\n",
        "        \n",
        "        #we choose one cell acccording to probabilities\n",
        "        new_state   = random.choices(cells, weights=p)[0] #only one; we take index 0                \n",
        "        reward      = self.reward(state, action) #\n",
        "        \n",
        "        is_terminal = False\n",
        "        if new_state == self.end_state :\n",
        "            is_terminal = True\n",
        "            \n",
        "        return new_state, reward, is_terminal #keep the same for mat as OpenAI gym.\n",
        "    \n",
        "    def _next_cell(self, state, action) : \n",
        "        \"\"\"Blindly takes the action without checking anything and returns the position\"\"\"\n",
        "        r,c = state #row & column\n",
        "        if action == Exit :\n",
        "            return self.end_state\n",
        "        if action == Up :\n",
        "            target = r-1, c  \n",
        "        if action == Down :\n",
        "            target = r+1, c\n",
        "        if action == Right :\n",
        "            target = r, c+1  \n",
        "        if action == Left :\n",
        "            target = r, c-1 \n",
        "        \n",
        "        if self._valid_cell(target) :\n",
        "            return target\n",
        "        return state #stay put the target is invalid.\n",
        "    \n",
        "    def _valid_cell(self, cell) :\n",
        "        \"\"\"Returns true if the cell is a valid cell\"\"\"\n",
        "        r, c = cell #this may be an illegal node; we need to check\n",
        "        \n",
        "        #is it any of the walls?\n",
        "        if (r,c) in self.walls :\n",
        "            return False\n",
        "        \n",
        "        #is it outside the grid?\n",
        "        if r < 0 or r >= self.rows or c < 0 or c >= self.columns :\n",
        "            return False\n",
        "        \n",
        "        return True\n",
        "    \n",
        "    #pretty print the grid and agent if given.\n",
        "    def print(self, agent_state=None) :\n",
        "        for r in range(self.rows) :\n",
        "            for c in range(self.columns) :\n",
        "                cell = (r,c)\n",
        "                if cell in self.walls :\n",
        "                    print('# ', end='')\n",
        "                elif cell in self.terminals :\n",
        "                    if self.terminals[cell] > 0 :\n",
        "                        print('+', end=' ')\n",
        "                    else :\n",
        "                        print('-', end=' ')\n",
        "                elif cell == agent_state :\n",
        "                    print('@ ', end='')\n",
        "                else :\n",
        "                    print('. ', end='')\n",
        "            print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jw8OLkqtvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMlt4WidlLos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_world = GridWorld(gamma=0.9, living_reward=-0.04)\n",
        "start = (2,0) #as in the book"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoZp-RtKlQEB",
        "colab_type": "code",
        "outputId": "3fc0f098-160d-4b48-ee21-48e82bf0eaea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# + and - are the terminal states. @ is our agent.\n",
        "grid_world.print(start)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ". . . + \n",
            ". # . - \n",
            "@ . . . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiIB6IdtlHJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This is a simple class to hold the policy dictionary\n",
        "#useful for printing the policy and hiding some details.\n",
        "\n",
        "class Policy :\n",
        "    def __init__(self, grid_world=None) :\n",
        "        \"\"\"Holds one policy and returns actions according to it\"\"\"\n",
        "        self.grid_world = grid_world\n",
        "        self.policy     = { } #{ state: policy_action}\n",
        "        \n",
        "    def __getitem__(self, state) :\n",
        "        return self.policy[state]\n",
        "    \n",
        "    def __setitem__(self, state, action) :\n",
        "        self.policy[state] = action\n",
        "    \n",
        "    \n",
        "    \n",
        "    #Just a pretty print function for easy debugging\n",
        "    def print(self) :\n",
        "        print_chars = {Up:'^', Down:'v', Right:'>', Left:'<', Exit:'+'}\n",
        "        for state in [(r,c) for r in range(grid_world.rows) for c in range(grid_world.columns)]:\n",
        "            \n",
        "            if state in self.grid_world.terminals :\n",
        "                if self.grid_world.terminals[state] >= 0 :\n",
        "                    print('+', end=' ') #positive reward terminal\n",
        "                else :\n",
        "                    print('-', end=' ') #-ve reward terminal\n",
        "                    \n",
        "            elif state not in self.policy :\n",
        "                print('#', end=' ') #walls\n",
        "            else :\n",
        "                print(print_chars[self.policy[state]], end=' ') #directions >, <, ^, v\n",
        "                \n",
        "            if (state[1]+1) % grid_world.columns == 0 :\n",
        "                print(\"\") #just a newline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkfY0MkRCP62",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################################################################\n",
        "# Now we implement some algorithms \n",
        "###################################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQWER6ecCQGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def qvalue(grid_world, state, action, V) :\n",
        "    \"\"\"returns the Q value of the state action pair\"\"\"\n",
        "    #  SUM [  P(s' | s, a) * ( R(s,a,s') + V(s2) ) ] of all s' from (s,a)\n",
        "    next_states, actions, p = grid_world.transitions(state, action) \n",
        "    gamma = grid_world.gamma\n",
        "    \n",
        "    values = [ p[i] * ( grid_world.reward(state, actions[i], s) + gamma*V[s] ) \n",
        "              for i,s in enumerate(next_states) ]\n",
        "    #print(values)\n",
        "    #print(sum(values))\n",
        "    return sum( values )\n",
        "\n",
        "def max_qvalue(grid_world, state, V) :\n",
        "    \"\"\"returns the maximum of q values and its action\"\"\"\n",
        "    q = [ (qvalue(grid_world, state, action, V), action) for action in grid_world.actions(state) ]\n",
        "    #print(q)\n",
        "    return max(q) #returns (value, action)\n",
        "\n",
        "def value_iteration(grid_world, N=1000) :\n",
        "    states = grid_world.all_states\n",
        "    #epsilon = 0.0001 * (1-gamma)/gamma\n",
        "    epsilon = 1e-10\n",
        "    \n",
        "    #initialize to 0\n",
        "    #U = { s: 0 for s in states }\n",
        "    V = { s: 0 for s in states }\n",
        "            \n",
        "    while True :\n",
        "        #we keep tracof the maximum value change\n",
        "        #if the maximum value change is less than a small value, epsilon, we can stop our iterations\n",
        "        max_delta = 0 \n",
        "        \n",
        "        for state in states :\n",
        "            if state != grid_world.end_state :\n",
        "                \n",
        "                qmax, qaction = max_qvalue(grid_world, state, V)\n",
        "\n",
        "                delta = abs(qmax - V[state])\n",
        "                max_delta = max( [ max_delta, delta] )#we keep them max of these tow values\n",
        "\n",
        "                #update the Values\n",
        "                V[state] = qmax\n",
        "        \n",
        "        #print(max_delta)\n",
        "        if max_delta < epsilon : #we are not improving much. Converged?\n",
        "            break\n",
        "            \n",
        "    return V\n",
        " \n",
        "def policy_from_value(grid_world, V) :\n",
        "    p = Policy(grid_world)\n",
        "    for state in grid_world.all_states :\n",
        "        if state != grid_world.end_state : #we dont have a policy for this state because no actions are valid\n",
        "            qmax, qaction = max_qvalue(grid_world, state, V)\n",
        "            p[state] = qaction\n",
        "    return p\n",
        "\n",
        "\n",
        "#example run\n",
        "# grid_world = GridWorld(gamma=0.9, living_reward=-0.04)\n",
        "# V = value_iteration(grid_world)\n",
        "# p = policy_from_value(grid_world, V)\n",
        "# p.print()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGiwdj6RCQaT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ref: http://incompleteideas.net/book/first/ebook/node43.html\n",
        "\n",
        "def policy_evaluation( grid_world, policy ) :\n",
        "    \"\"\"This will run value iteration until convergence and return the converged Values\"\"\"\n",
        "    states = grid_world.all_states\n",
        "    gamma  = grid_world.gamma\n",
        "    #epsilon = 0.0001 * (1-gamma)/gamma\n",
        "    epsilon = 1e-7\n",
        "    \n",
        "    V = { s: 0 for s in states }\n",
        "            \n",
        "    while True : #we exit when less than epsilon diff is made\n",
        "        max_delta = 0\n",
        "        \n",
        "        for state in states :            \n",
        "            if state != grid_world.end_state : #we dont have a policy for this state because no actions are valid\n",
        "                \n",
        "                action = policy[state] #we run this policy            \n",
        "                q      = qvalue(grid_world, state, action, V)           \n",
        "                #print(q)\n",
        "\n",
        "                delta = abs(q - V[state])\n",
        "                max_delta = max( [ max_delta, delta] )#we keep them max of these tow values\n",
        "\n",
        "                V[state] = q\n",
        "            \n",
        "        if max_delta < epsilon : #we are not improving much. Converged?\n",
        "            break\n",
        "                \n",
        "    return V\n",
        "\n",
        "\n",
        "def policy_improvement(grid_world, policy) :\n",
        "    \"\"\"Returns the new improved policy\"\"\"\n",
        "    \n",
        "    while True :\n",
        "        improving = False\n",
        "        \n",
        "        #find the values for this policy\n",
        "        V = policy_evaluation( grid_world, policy )\n",
        "        \n",
        "        #find the policy according to the new values we got\n",
        "        new_policy = policy_from_value(grid_world, V)\n",
        "    \n",
        "        for state in grid_world.all_states :\n",
        "            if state != grid_world.end_state : #we dont have a policy for this state because no actions are valid\n",
        "\n",
        "                if policy[state] != new_policy[state] : #Do we have an improvement?\n",
        "                    improving = True\n",
        "                \n",
        "        if not improving:\n",
        "            return policy\n",
        "            break\n",
        "            \n",
        "        policy = new_policy\n",
        "        \n",
        "        #for debug\n",
        "        #print('_______')\n",
        "        #policy.print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8uac7g9lb46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###############################################################################\n",
        "###### Some test code. ########################################################\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh2ZfGg2logH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_policy(grid_world) :\n",
        "    \n",
        "    #we need to choose a random action every time the policy is accessed\n",
        "    #here we overload the getitem \n",
        "    #when the user says policy[state] they get a random action\n",
        "    class _RandomPolicy(Policy) :\n",
        "        def __getitem__(self, state) :\n",
        "            return random.choice(grid_world.actions(state))\n",
        "    \n",
        "    p = _RandomPolicy(grid_world) \n",
        "    return p\n",
        "\n",
        "def fixed_policy(grid_world) :\n",
        "    p = Policy(grid_world)\n",
        "    p.policy = {state: Up for state in grid_world.all_states if state != grid_world.end_state }\n",
        "    p.policy.update({state:Exit for state in grid_world.terminals})\n",
        "    #print(p.policy)\n",
        "    return p\n",
        "\n",
        "def good_policy(grid_world) :\n",
        "    p = Policy(grid_world)\n",
        "    p.policy = {\n",
        "        (0,0):Right, (0,1): Right, (0,2): Right, (0,3) : Exit,\n",
        "        (1,0):Up,    (1,1): Right, (1,2): Up,    (1,3) : Exit,\n",
        "        (2,0):Up,    (2,1): Left,  (2,2): Up,    (2,3) : Left,\n",
        "               }\n",
        "    p.policy.update({state:Exit for state in grid_world.terminals})\n",
        "    #print(p.policy)\n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSciC2EJ-uXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(grid_world, state, policy=None) :\n",
        "    \"\"\"runs a full episode and return the total reward\"\"\"\n",
        "    rewards = []\n",
        "    gamma = grid_world.gamma\n",
        "    \n",
        "    time_step = 0\n",
        "    while True :\n",
        "        action = policy[state]\n",
        "        #a.print()\n",
        "        #print(action)\n",
        "        state, r, exited = grid_world.move(state, action)\n",
        "        rewards.append(r * (gamma**time_step) ) #the further we go down, the less we value the reward\n",
        "        if exited :\n",
        "            break    \n",
        "        time_step += 1\n",
        "    return rewards\n",
        "\n",
        "\n",
        "def expected_utility(grid_world, state, policy, N=100) :\n",
        "    \"\"\"run the policy till completion several times and return the expected utility\"\"\"\n",
        "    s = 0.0\n",
        "    for _ in range(N) :\n",
        "        #from the same start state we run till completion, N times\n",
        "        s += sum( run(grid_world, state, policy) )\n",
        "    return s/N"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te1GnYPLlpNz",
        "colab_type": "code",
        "outputId": "157253fc-a83e-4a13-dd83-271746c2fa7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#page  651; AIMA Book\n",
        "#The utilities of the states in the 4 × 3 world, calculated with γ = 1 and\n",
        "#R(s) = − 0.04 for nonterminal states\n",
        "\n",
        "\n",
        "\n",
        "N = 1000\n",
        "\n",
        "grid_world = GridWorld(gamma=1.0, living_reward=-0.04)\n",
        "policy = good_policy(grid_world)\n",
        "policy.print()\n",
        "\n",
        "for state in grid_world.all_states :\n",
        "    if state != grid_world.end_state :\n",
        "        print( expected_utility(grid_world, state, policy, N) )"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> > > + \n",
            "^ > ^ - \n",
            "^ < ^ < \n",
            "0.8248799999999964\n",
            "0.8709599999999894\n",
            "0.9120000000000072\n",
            "1.0\n",
            "0.7684399999999975\n",
            "0.6493999999999963\n",
            "-1.0\n",
            "0.6962799999999985\n",
            "0.66068\n",
            "0.5863199999999982\n",
            "0.3808399999999971\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRky_dO4DEp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### Lets run some value iteration and check the policy from it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18mKvMgytNh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_world = GridWorld(gamma=0.9, living_reward=-0.04)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNafgb8NtR2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "V = value_iteration(grid_world)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li5DOvh4tULe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = policy_from_value(grid_world, V)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLWHT7wHuHPk",
        "colab_type": "code",
        "outputId": "cc405e68-e064-41f1-f182-f7d72ef019dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "p.print()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> > > + \n",
            "^ # ^ - \n",
            "^ > ^ < \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDc3BXIluIUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####### try policy iteration"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RphinsleAMwB",
        "colab_type": "code",
        "outputId": "b5c84e67-4fd9-4bef-c5dc-2f1bdb369de8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "\n",
        "grid_world = GridWorld(gamma=0.5, living_reward=-0.05)\n",
        "\n",
        "#Before policy\n",
        "p = fixed_policy(grid_world)\n",
        "\n",
        "#new policy\n",
        "newp = policy_improvement(grid_world, p)\n",
        "\n",
        "\n",
        "p.print()\n",
        "print('_____')\n",
        "newp.print()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "^ ^ ^ + \n",
            "^ # ^ - \n",
            "^ ^ ^ ^ \n",
            "_____\n",
            "> > > + \n",
            "^ # ^ - \n",
            "^ > ^ v \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYBNb0mvAPp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}